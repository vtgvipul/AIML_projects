{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1738,
     "status": "ok",
     "timestamp": 1574342802315,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "yeUlXwljJF8x",
    "outputId": "f613ea8c-1361-400c-9b03-482358bf40ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3m64izx05_VU"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = pd.read_csv('complete_dataSet_NN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fund_id</th>\n",
       "      <th>10years_category_r_squared</th>\n",
       "      <th>10yrs_sharpe_ratio_fund</th>\n",
       "      <th>10_years_alpha_fund</th>\n",
       "      <th>10years_fund_r_squared</th>\n",
       "      <th>10years_fund_std</th>\n",
       "      <th>10yrs_sharpe_ratio_category</th>\n",
       "      <th>10_years_beta_fund</th>\n",
       "      <th>10yrs_treynor_ratio_fund</th>\n",
       "      <th>10_years_return_mean_annual_category</th>\n",
       "      <th>...</th>\n",
       "      <th>5yrs_treynor_ratio_fund</th>\n",
       "      <th>5_years_return_mean_annual_fund</th>\n",
       "      <th>5_years_return_mean_annual_category</th>\n",
       "      <th>5yrs_treynor_ratio_category</th>\n",
       "      <th>5_years_return_fund</th>\n",
       "      <th>5_years_alpha_category</th>\n",
       "      <th>5_years_beta_category</th>\n",
       "      <th>5years_category_std</th>\n",
       "      <th>5_years_return_category</th>\n",
       "      <th>greatstone_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f5ad58c2-fdea-4087-8678-e04744f89f90</td>\n",
       "      <td>0.543477</td>\n",
       "      <td>8.022707e-01</td>\n",
       "      <td>6.146698e-02</td>\n",
       "      <td>0.724021</td>\n",
       "      <td>0.598922</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>9.106054e-02</td>\n",
       "      <td>0.045959</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>2.524637e-01</td>\n",
       "      <td>1.760082e+00</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>1.821475</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3c13f4ab-02c4-4ca7-a133-7e996ec5d0c4</td>\n",
       "      <td>0.543477</td>\n",
       "      <td>1.018608e+00</td>\n",
       "      <td>3.779429e-01</td>\n",
       "      <td>0.676481</td>\n",
       "      <td>0.262188</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>-1.031006e-02</td>\n",
       "      <td>0.054764</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534772e-17</td>\n",
       "      <td>1.951215e+00</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>2.078089</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ff78bdd8-59eb-4cef-9f3c-b1baacce9554</td>\n",
       "      <td>0.616711</td>\n",
       "      <td>9.464956e-01</td>\n",
       "      <td>2.837536e-01</td>\n",
       "      <td>0.596766</td>\n",
       "      <td>-0.043367</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>-9.608518e-02</td>\n",
       "      <td>0.053021</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>2.084051e-01</td>\n",
       "      <td>5.814308e-01</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.751849</td>\n",
       "      <td>-0.830080</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.340467</td>\n",
       "      <td>0.718129</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63d8406d-c525-494a-8e03-d4fc4cfcb571</td>\n",
       "      <td>0.873030</td>\n",
       "      <td>-4.003045e-16</td>\n",
       "      <td>-1.879012e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>3.657189e-17</td>\n",
       "      <td>3.536665e-16</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.986893</td>\n",
       "      <td>-0.071294</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.063623</td>\n",
       "      <td>0.350640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>090afb4d-eca3-4f69-8275-a140a0b40292</td>\n",
       "      <td>0.360393</td>\n",
       "      <td>8.114626e-02</td>\n",
       "      <td>-2.437062e-01</td>\n",
       "      <td>0.364345</td>\n",
       "      <td>-0.120275</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>2.678746e-17</td>\n",
       "      <td>-0.016200</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.741734e-02</td>\n",
       "      <td>2.310209e-01</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>0.368127</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.063623</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fund_id  10years_category_r_squared  \\\n",
       "0  f5ad58c2-fdea-4087-8678-e04744f89f90                    0.543477   \n",
       "1  3c13f4ab-02c4-4ca7-a133-7e996ec5d0c4                    0.543477   \n",
       "2  ff78bdd8-59eb-4cef-9f3c-b1baacce9554                    0.616711   \n",
       "3  63d8406d-c525-494a-8e03-d4fc4cfcb571                    0.873030   \n",
       "4  090afb4d-eca3-4f69-8275-a140a0b40292                    0.360393   \n",
       "\n",
       "   10yrs_sharpe_ratio_fund  10_years_alpha_fund  10years_fund_r_squared  \\\n",
       "0             8.022707e-01         6.146698e-02                0.724021   \n",
       "1             1.018608e+00         3.779429e-01                0.676481   \n",
       "2             9.464956e-01         2.837536e-01                0.596766   \n",
       "3            -4.003045e-16        -1.879012e-17                0.000000   \n",
       "4             8.114626e-02        -2.437062e-01                0.364345   \n",
       "\n",
       "   10years_fund_std  10yrs_sharpe_ratio_category  10_years_beta_fund  \\\n",
       "0          0.598922                      0.21756        9.106054e-02   \n",
       "1          0.262188                      0.21756       -1.031006e-02   \n",
       "2         -0.043367                      0.21756       -9.608518e-02   \n",
       "3          0.000000                      0.21756        0.000000e+00   \n",
       "4         -0.120275                      0.21756        2.678746e-17   \n",
       "\n",
       "   10yrs_treynor_ratio_fund  10_years_return_mean_annual_category  ...  \\\n",
       "0                  0.045959                              0.514846  ...   \n",
       "1                  0.054764                              0.514846  ...   \n",
       "2                  0.053021                              0.514846  ...   \n",
       "3                  0.000000                              0.514846  ...   \n",
       "4                 -0.016200                              0.514846  ...   \n",
       "\n",
       "   5yrs_treynor_ratio_fund  5_years_return_mean_annual_fund  \\\n",
       "0             2.524637e-01                     1.760082e+00   \n",
       "1             1.534772e-17                     1.951215e+00   \n",
       "2             2.084051e-01                     5.814308e-01   \n",
       "3             3.657189e-17                     3.536665e-16   \n",
       "4            -3.741734e-02                     2.310209e-01   \n",
       "\n",
       "   5_years_return_mean_annual_category  5yrs_treynor_ratio_category  \\\n",
       "0                             1.207049                     1.333001   \n",
       "1                             1.207049                     1.333001   \n",
       "2                             1.207049                     0.655556   \n",
       "3                             1.207049                    -0.021888   \n",
       "4                             1.207049                    -0.021888   \n",
       "\n",
       "   5_years_return_fund  5_years_alpha_category  5_years_beta_category  \\\n",
       "0             1.821475                0.308098               0.347165   \n",
       "1             2.078089                0.308098               0.347165   \n",
       "2             0.751849               -0.830080               0.347165   \n",
       "3            -0.986893               -0.071294               0.347165   \n",
       "4             0.368127                0.308098               0.347165   \n",
       "\n",
       "   5years_category_std  5_years_return_category  greatstone_rating  \n",
       "0             0.744557                 1.911734                3.0  \n",
       "1             0.744557                 1.911734                4.0  \n",
       "2             0.340467                 0.718129                3.0  \n",
       "3            -0.063623                 0.350640                0.0  \n",
       "4            -0.063623                 0.333001                3.0  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.drop(['fund_id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 105)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet.fillna(dataSet.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10years_category_r_squared</th>\n",
       "      <th>10yrs_sharpe_ratio_fund</th>\n",
       "      <th>10_years_alpha_fund</th>\n",
       "      <th>10years_fund_r_squared</th>\n",
       "      <th>10years_fund_std</th>\n",
       "      <th>10yrs_sharpe_ratio_category</th>\n",
       "      <th>10_years_beta_fund</th>\n",
       "      <th>10yrs_treynor_ratio_fund</th>\n",
       "      <th>10_years_return_mean_annual_category</th>\n",
       "      <th>10yrs_treynor_ratio_category</th>\n",
       "      <th>...</th>\n",
       "      <th>5yrs_treynor_ratio_fund</th>\n",
       "      <th>5_years_return_mean_annual_fund</th>\n",
       "      <th>5_years_return_mean_annual_category</th>\n",
       "      <th>5yrs_treynor_ratio_category</th>\n",
       "      <th>5_years_return_fund</th>\n",
       "      <th>5_years_alpha_category</th>\n",
       "      <th>5_years_beta_category</th>\n",
       "      <th>5years_category_std</th>\n",
       "      <th>5_years_return_category</th>\n",
       "      <th>greatstone_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.543477</td>\n",
       "      <td>8.022707e-01</td>\n",
       "      <td>6.146698e-02</td>\n",
       "      <td>0.724021</td>\n",
       "      <td>0.598922</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>9.106054e-02</td>\n",
       "      <td>0.045959</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>...</td>\n",
       "      <td>2.524637e-01</td>\n",
       "      <td>1.760082e+00</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>1.821475</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.543477</td>\n",
       "      <td>1.018608e+00</td>\n",
       "      <td>3.779429e-01</td>\n",
       "      <td>0.676481</td>\n",
       "      <td>0.262188</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>-1.031006e-02</td>\n",
       "      <td>0.054764</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>...</td>\n",
       "      <td>1.534772e-17</td>\n",
       "      <td>1.951215e+00</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>2.078089</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.616711</td>\n",
       "      <td>9.464956e-01</td>\n",
       "      <td>2.837536e-01</td>\n",
       "      <td>0.596766</td>\n",
       "      <td>-0.043367</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>-9.608518e-02</td>\n",
       "      <td>0.053021</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>...</td>\n",
       "      <td>2.084051e-01</td>\n",
       "      <td>5.814308e-01</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>0.655556</td>\n",
       "      <td>0.751849</td>\n",
       "      <td>-0.830080</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.340467</td>\n",
       "      <td>0.718129</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.873030</td>\n",
       "      <td>-4.003045e-16</td>\n",
       "      <td>-1.879012e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.133238</td>\n",
       "      <td>...</td>\n",
       "      <td>3.657189e-17</td>\n",
       "      <td>3.536665e-16</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.986893</td>\n",
       "      <td>-0.071294</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.063623</td>\n",
       "      <td>0.350640</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.360393</td>\n",
       "      <td>8.114626e-02</td>\n",
       "      <td>-2.437062e-01</td>\n",
       "      <td>0.364345</td>\n",
       "      <td>-0.120275</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>2.678746e-17</td>\n",
       "      <td>-0.016200</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.110593</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.741734e-02</td>\n",
       "      <td>2.310209e-01</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>0.368127</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.063623</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10years_category_r_squared  10yrs_sharpe_ratio_fund  10_years_alpha_fund  \\\n",
       "0                    0.543477             8.022707e-01         6.146698e-02   \n",
       "1                    0.543477             1.018608e+00         3.779429e-01   \n",
       "2                    0.616711             9.464956e-01         2.837536e-01   \n",
       "3                    0.873030            -4.003045e-16        -1.879012e-17   \n",
       "4                    0.360393             8.114626e-02        -2.437062e-01   \n",
       "\n",
       "   10years_fund_r_squared  10years_fund_std  10yrs_sharpe_ratio_category  \\\n",
       "0                0.724021          0.598922                      0.21756   \n",
       "1                0.676481          0.262188                      0.21756   \n",
       "2                0.596766         -0.043367                      0.21756   \n",
       "3                0.000000          0.000000                      0.21756   \n",
       "4                0.364345         -0.120275                      0.21756   \n",
       "\n",
       "   10_years_beta_fund  10yrs_treynor_ratio_fund  \\\n",
       "0        9.106054e-02                  0.045959   \n",
       "1       -1.031006e-02                  0.054764   \n",
       "2       -9.608518e-02                  0.053021   \n",
       "3        0.000000e+00                  0.000000   \n",
       "4        2.678746e-17                 -0.016200   \n",
       "\n",
       "   10_years_return_mean_annual_category  10yrs_treynor_ratio_category  ...  \\\n",
       "0                              0.514846                      0.025274  ...   \n",
       "1                              0.514846                      0.025274  ...   \n",
       "2                              0.514846                      0.002630  ...   \n",
       "3                              0.514846                     -0.133238  ...   \n",
       "4                              0.514846                     -0.110593  ...   \n",
       "\n",
       "   5yrs_treynor_ratio_fund  5_years_return_mean_annual_fund  \\\n",
       "0             2.524637e-01                     1.760082e+00   \n",
       "1             1.534772e-17                     1.951215e+00   \n",
       "2             2.084051e-01                     5.814308e-01   \n",
       "3             3.657189e-17                     3.536665e-16   \n",
       "4            -3.741734e-02                     2.310209e-01   \n",
       "\n",
       "   5_years_return_mean_annual_category  5yrs_treynor_ratio_category  \\\n",
       "0                             1.207049                     1.333001   \n",
       "1                             1.207049                     1.333001   \n",
       "2                             1.207049                     0.655556   \n",
       "3                             1.207049                    -0.021888   \n",
       "4                             1.207049                    -0.021888   \n",
       "\n",
       "   5_years_return_fund  5_years_alpha_category  5_years_beta_category  \\\n",
       "0             1.821475                0.308098               0.347165   \n",
       "1             2.078089                0.308098               0.347165   \n",
       "2             0.751849               -0.830080               0.347165   \n",
       "3            -0.986893               -0.071294               0.347165   \n",
       "4             0.368127                0.308098               0.347165   \n",
       "\n",
       "   5years_category_std  5_years_return_category  greatstone_rating  \n",
       "0             0.744557                 1.911734                3.0  \n",
       "1             0.744557                 1.911734                4.0  \n",
       "2             0.340467                 0.718129                3.0  \n",
       "3            -0.063623                 0.350640                0.0  \n",
       "4            -0.063623                 0.333001                3.0  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_vals = dataSet['greatstone_rating']\n",
    "X_vals = dataSet.drop('greatstone_rating', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.apply(zscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train.replace({0:y_train.median()}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        3.0\n",
       "1        4.0\n",
       "2        3.0\n",
       "3        0.0\n",
       "4        3.0\n",
       "        ... \n",
       "19995    2.0\n",
       "19996    2.0\n",
       "19997    3.0\n",
       "19998    1.0\n",
       "19999    1.0\n",
       "Name: greatstone_rating, Length: 20000, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,x_test,y_train,y_test = train_test_split(X_vals,Y_vals, random_state = 1, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data_NN_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fund_id</th>\n",
       "      <th>10years_category_r_squared_y</th>\n",
       "      <th>10yrs_sharpe_ratio_fund_y</th>\n",
       "      <th>10_years_alpha_fund_y</th>\n",
       "      <th>10years_fund_r_squared_y</th>\n",
       "      <th>10years_fund_std_y</th>\n",
       "      <th>10yrs_sharpe_ratio_category_y</th>\n",
       "      <th>10_years_beta_fund_y</th>\n",
       "      <th>10yrs_treynor_ratio_fund_y</th>\n",
       "      <th>10_years_return_mean_annual_category_y</th>\n",
       "      <th>...</th>\n",
       "      <th>5yrs_treynor_ratio_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_category_y</th>\n",
       "      <th>5yrs_treynor_ratio_category_y</th>\n",
       "      <th>5_years_return_fund_y</th>\n",
       "      <th>5_years_alpha_category_y</th>\n",
       "      <th>5_years_beta_category_y</th>\n",
       "      <th>5years_category_std_y</th>\n",
       "      <th>5_years_return_category_y</th>\n",
       "      <th>greatstone_rating_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85a6edf9-db47-490c-981a-168ce90434bb</td>\n",
       "      <td>0.689945</td>\n",
       "      <td>7.662144e-01</td>\n",
       "      <td>4.639670e-02</td>\n",
       "      <td>0.985255</td>\n",
       "      <td>-1.612714</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>-0.051421</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.441731e-02</td>\n",
       "      <td>-4.697990e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.247703</td>\n",
       "      <td>-0.135509</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.469596</td>\n",
       "      <td>2.83975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>654dac08-5e5d-4cf0-870e-3167087de7d2</td>\n",
       "      <td>0.281792</td>\n",
       "      <td>1.172025e-01</td>\n",
       "      <td>-5.463049e-02</td>\n",
       "      <td>-0.322354</td>\n",
       "      <td>0.104214</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>0.049533</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>5.069990e-02</td>\n",
       "      <td>-2.382270e-02</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>0.135495</td>\n",
       "      <td>0.038507</td>\n",
       "      <td>0.262340</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>1.209095</td>\n",
       "      <td>2.83975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75214019-f876-42a9-a316-328b759ac4ba</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391881</td>\n",
       "      <td>...</td>\n",
       "      <td>2.224050e-01</td>\n",
       "      <td>-7.564980e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>-0.411309</td>\n",
       "      <td>0.687491</td>\n",
       "      <td>0.281527</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.798866</td>\n",
       "      <td>2.83975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1fe8de6d-cd33-489e-a829-211f1f622eed</td>\n",
       "      <td>0.763179</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>3.660000e-17</td>\n",
       "      <td>3.540000e-16</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.986893</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.669758</td>\n",
       "      <td>-0.052128</td>\n",
       "      <td>2.83975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bd87a7a0-6ca5-4607-a0cf-d396ecd6037a</td>\n",
       "      <td>0.543477</td>\n",
       "      <td>4.417085e-01</td>\n",
       "      <td>-1.721224e-01</td>\n",
       "      <td>0.312962</td>\n",
       "      <td>0.702853</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.075465</td>\n",
       "      <td>0.039072</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>...</td>\n",
       "      <td>2.726401e-01</td>\n",
       "      <td>-7.208662e-03</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>2.092479</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "      <td>2.83975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                fund_id  10years_category_r_squared_y  \\\n",
       "0  85a6edf9-db47-490c-981a-168ce90434bb                      0.689945   \n",
       "1  654dac08-5e5d-4cf0-870e-3167087de7d2                      0.281792   \n",
       "2  75214019-f876-42a9-a316-328b759ac4ba                      0.302511   \n",
       "3  1fe8de6d-cd33-489e-a829-211f1f622eed                      0.763179   \n",
       "4  bd87a7a0-6ca5-4607-a0cf-d396ecd6037a                      0.543477   \n",
       "\n",
       "   10yrs_sharpe_ratio_fund_y  10_years_alpha_fund_y  10years_fund_r_squared_y  \\\n",
       "0               7.662144e-01           4.639670e-02                  0.985255   \n",
       "1               1.172025e-01          -5.463049e-02                 -0.322354   \n",
       "2              -4.000000e-16          -1.880000e-17                  0.000000   \n",
       "3              -4.000000e-16          -1.880000e-17                  0.000000   \n",
       "4               4.417085e-01          -1.721224e-01                  0.312962   \n",
       "\n",
       "   10years_fund_std_y  10yrs_sharpe_ratio_category_y  10_years_beta_fund_y  \\\n",
       "0           -1.612714                        0.21756              0.005285   \n",
       "1            0.104214                        0.21756              0.018901   \n",
       "2            0.000000                        0.21756              0.000000   \n",
       "3            0.000000                        0.21756              0.000000   \n",
       "4            0.702853                        0.21756              0.075465   \n",
       "\n",
       "   10yrs_treynor_ratio_fund_y  10_years_return_mean_annual_category_y  ...  \\\n",
       "0                   -0.051421                                0.167234  ...   \n",
       "1                    0.049533                                0.514846  ...   \n",
       "2                    0.000000                                0.391881  ...   \n",
       "3                    0.000000                                0.514846  ...   \n",
       "4                    0.039072                                0.514846  ...   \n",
       "\n",
       "   5yrs_treynor_ratio_fund_y  5_years_return_mean_annual_fund_y  \\\n",
       "0              -4.441731e-02                      -4.697990e-01   \n",
       "1               5.069990e-02                      -2.382270e-02   \n",
       "2               2.224050e-01                      -7.564980e-01   \n",
       "3               3.660000e-17                       3.540000e-16   \n",
       "4               2.726401e-01                      -7.208662e-03   \n",
       "\n",
       "   5_years_return_mean_annual_category_y  5yrs_treynor_ratio_category_y  \\\n",
       "0                              -0.770071                      -0.247703   \n",
       "1                               1.207049                       0.065172   \n",
       "2                              -0.770071                       0.070614   \n",
       "3                              -0.770071                      -0.021888   \n",
       "4                               1.207049                       1.333001   \n",
       "\n",
       "   5_years_return_fund_y  5_years_alpha_category_y  5_years_beta_category_y  \\\n",
       "0              -0.135509                  0.308098                 0.347165   \n",
       "1               0.135495                  0.038507                 0.262340   \n",
       "2              -0.411309                  0.687491                 0.281527   \n",
       "3              -0.986893                  0.308098                 0.347165   \n",
       "4               2.092479                  0.308098                 0.347165   \n",
       "\n",
       "   5years_category_std_y  5_years_return_category_y  greatstone_rating_y  \n",
       "0              -1.275893                  -0.469596              2.83975  \n",
       "1               0.542512                   1.209095              2.83975  \n",
       "2              -1.275893                  -0.798866              2.83975  \n",
       "3              -0.669758                  -0.052128              2.83975  \n",
       "4               0.744557                   1.911734              2.83975  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(['greatstone_rating_y','fund_id'],axis =1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10years_category_r_squared_y</th>\n",
       "      <th>10yrs_sharpe_ratio_fund_y</th>\n",
       "      <th>10_years_alpha_fund_y</th>\n",
       "      <th>10years_fund_r_squared_y</th>\n",
       "      <th>10years_fund_std_y</th>\n",
       "      <th>10yrs_sharpe_ratio_category_y</th>\n",
       "      <th>10_years_beta_fund_y</th>\n",
       "      <th>10yrs_treynor_ratio_fund_y</th>\n",
       "      <th>10_years_return_mean_annual_category_y</th>\n",
       "      <th>10yrs_treynor_ratio_category_y</th>\n",
       "      <th>...</th>\n",
       "      <th>5_years_beta_fund_y</th>\n",
       "      <th>5yrs_treynor_ratio_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_category_y</th>\n",
       "      <th>5yrs_treynor_ratio_category_y</th>\n",
       "      <th>5_years_return_fund_y</th>\n",
       "      <th>5_years_alpha_category_y</th>\n",
       "      <th>5_years_beta_category_y</th>\n",
       "      <th>5years_category_std_y</th>\n",
       "      <th>5_years_return_category_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.689945</td>\n",
       "      <td>7.662144e-01</td>\n",
       "      <td>4.639670e-02</td>\n",
       "      <td>0.985255</td>\n",
       "      <td>-1.612714</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>-0.051421</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>-0.223817</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.716087e-02</td>\n",
       "      <td>-4.441731e-02</td>\n",
       "      <td>-4.697990e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.247703</td>\n",
       "      <td>-0.135509</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.469596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281792</td>\n",
       "      <td>1.172025e-01</td>\n",
       "      <td>-5.463049e-02</td>\n",
       "      <td>-0.322354</td>\n",
       "      <td>0.104214</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>0.049533</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.179938e-01</td>\n",
       "      <td>5.069990e-02</td>\n",
       "      <td>-2.382270e-02</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>0.135495</td>\n",
       "      <td>0.038507</td>\n",
       "      <td>0.262340</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>1.209095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.302511</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391881</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>...</td>\n",
       "      <td>9.006177e-02</td>\n",
       "      <td>2.224050e-01</td>\n",
       "      <td>-7.564980e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>-0.411309</td>\n",
       "      <td>0.687491</td>\n",
       "      <td>0.281527</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.798866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.763179</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.110593</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.880000e-16</td>\n",
       "      <td>3.660000e-17</td>\n",
       "      <td>3.540000e-16</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.986893</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.669758</td>\n",
       "      <td>-0.052128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.543477</td>\n",
       "      <td>4.417085e-01</td>\n",
       "      <td>-1.721224e-01</td>\n",
       "      <td>0.312962</td>\n",
       "      <td>0.702853</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.075465</td>\n",
       "      <td>0.039072</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>...</td>\n",
       "      <td>3.590886e-01</td>\n",
       "      <td>2.726401e-01</td>\n",
       "      <td>-7.208662e-03</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>2.092479</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10years_category_r_squared_y  10yrs_sharpe_ratio_fund_y  \\\n",
       "0                      0.689945               7.662144e-01   \n",
       "1                      0.281792               1.172025e-01   \n",
       "2                      0.302511              -4.000000e-16   \n",
       "3                      0.763179              -4.000000e-16   \n",
       "4                      0.543477               4.417085e-01   \n",
       "\n",
       "   10_years_alpha_fund_y  10years_fund_r_squared_y  10years_fund_std_y  \\\n",
       "0           4.639670e-02                  0.985255           -1.612714   \n",
       "1          -5.463049e-02                 -0.322354            0.104214   \n",
       "2          -1.880000e-17                  0.000000            0.000000   \n",
       "3          -1.880000e-17                  0.000000            0.000000   \n",
       "4          -1.721224e-01                  0.312962            0.702853   \n",
       "\n",
       "   10yrs_sharpe_ratio_category_y  10_years_beta_fund_y  \\\n",
       "0                        0.21756              0.005285   \n",
       "1                        0.21756              0.018901   \n",
       "2                        0.21756              0.000000   \n",
       "3                        0.21756              0.000000   \n",
       "4                        0.21756              0.075465   \n",
       "\n",
       "   10yrs_treynor_ratio_fund_y  10_years_return_mean_annual_category_y  \\\n",
       "0                   -0.051421                                0.167234   \n",
       "1                    0.049533                                0.514846   \n",
       "2                    0.000000                                0.391881   \n",
       "3                    0.000000                                0.514846   \n",
       "4                    0.039072                                0.514846   \n",
       "\n",
       "   10yrs_treynor_ratio_category_y  ...  5_years_beta_fund_y  \\\n",
       "0                       -0.223817  ...        -4.716087e-02   \n",
       "1                       -0.069452  ...        -3.179938e-01   \n",
       "2                        0.002630  ...         9.006177e-02   \n",
       "3                       -0.110593  ...        -1.880000e-16   \n",
       "4                        0.025274  ...         3.590886e-01   \n",
       "\n",
       "   5yrs_treynor_ratio_fund_y  5_years_return_mean_annual_fund_y  \\\n",
       "0              -4.441731e-02                      -4.697990e-01   \n",
       "1               5.069990e-02                      -2.382270e-02   \n",
       "2               2.224050e-01                      -7.564980e-01   \n",
       "3               3.660000e-17                       3.540000e-16   \n",
       "4               2.726401e-01                      -7.208662e-03   \n",
       "\n",
       "   5_years_return_mean_annual_category_y  5yrs_treynor_ratio_category_y  \\\n",
       "0                              -0.770071                      -0.247703   \n",
       "1                               1.207049                       0.065172   \n",
       "2                              -0.770071                       0.070614   \n",
       "3                              -0.770071                      -0.021888   \n",
       "4                               1.207049                       1.333001   \n",
       "\n",
       "   5_years_return_fund_y  5_years_alpha_category_y  5_years_beta_category_y  \\\n",
       "0              -0.135509                  0.308098                 0.347165   \n",
       "1               0.135495                  0.038507                 0.262340   \n",
       "2              -0.411309                  0.687491                 0.281527   \n",
       "3              -0.986893                  0.308098                 0.347165   \n",
       "4               2.092479                  0.308098                 0.347165   \n",
       "\n",
       "   5years_category_std_y  5_years_return_category_y  \n",
       "0              -1.275893                  -0.469596  \n",
       "1               0.542512                   1.209095  \n",
       "2              -1.275893                  -0.798866  \n",
       "3              -0.669758                  -0.052128  \n",
       "4               0.744557                   1.911734  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data.fillna(test_data.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10years_category_r_squared_y</th>\n",
       "      <th>10yrs_sharpe_ratio_fund_y</th>\n",
       "      <th>10_years_alpha_fund_y</th>\n",
       "      <th>10years_fund_r_squared_y</th>\n",
       "      <th>10years_fund_std_y</th>\n",
       "      <th>10yrs_sharpe_ratio_category_y</th>\n",
       "      <th>10_years_beta_fund_y</th>\n",
       "      <th>10yrs_treynor_ratio_fund_y</th>\n",
       "      <th>10_years_return_mean_annual_category_y</th>\n",
       "      <th>10yrs_treynor_ratio_category_y</th>\n",
       "      <th>...</th>\n",
       "      <th>5_years_beta_fund_y</th>\n",
       "      <th>5yrs_treynor_ratio_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_category_y</th>\n",
       "      <th>5yrs_treynor_ratio_category_y</th>\n",
       "      <th>5_years_return_fund_y</th>\n",
       "      <th>5_years_alpha_category_y</th>\n",
       "      <th>5_years_beta_category_y</th>\n",
       "      <th>5years_category_std_y</th>\n",
       "      <th>5_years_return_category_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.689945</td>\n",
       "      <td>7.662144e-01</td>\n",
       "      <td>4.639670e-02</td>\n",
       "      <td>0.985255</td>\n",
       "      <td>-1.612714</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>-0.051421</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>-0.223817</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.716087e-02</td>\n",
       "      <td>-4.441731e-02</td>\n",
       "      <td>-4.697990e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.247703</td>\n",
       "      <td>-0.135509</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.469596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281792</td>\n",
       "      <td>1.172025e-01</td>\n",
       "      <td>-5.463049e-02</td>\n",
       "      <td>-0.322354</td>\n",
       "      <td>0.104214</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>0.049533</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.179938e-01</td>\n",
       "      <td>5.069990e-02</td>\n",
       "      <td>-2.382270e-02</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>0.135495</td>\n",
       "      <td>0.038507</td>\n",
       "      <td>0.262340</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>1.209095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.302511</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391881</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>...</td>\n",
       "      <td>9.006177e-02</td>\n",
       "      <td>2.224050e-01</td>\n",
       "      <td>-7.564980e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>-0.411309</td>\n",
       "      <td>0.687491</td>\n",
       "      <td>0.281527</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.798866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.763179</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.110593</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.880000e-16</td>\n",
       "      <td>3.660000e-17</td>\n",
       "      <td>3.540000e-16</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.986893</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.669758</td>\n",
       "      <td>-0.052128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.543477</td>\n",
       "      <td>4.417085e-01</td>\n",
       "      <td>-1.721224e-01</td>\n",
       "      <td>0.312962</td>\n",
       "      <td>0.702853</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.075465</td>\n",
       "      <td>0.039072</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>...</td>\n",
       "      <td>3.590886e-01</td>\n",
       "      <td>2.726401e-01</td>\n",
       "      <td>-7.208662e-03</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>2.092479</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10years_category_r_squared_y  10yrs_sharpe_ratio_fund_y  \\\n",
       "0                      0.689945               7.662144e-01   \n",
       "1                      0.281792               1.172025e-01   \n",
       "2                      0.302511              -4.000000e-16   \n",
       "3                      0.763179              -4.000000e-16   \n",
       "4                      0.543477               4.417085e-01   \n",
       "\n",
       "   10_years_alpha_fund_y  10years_fund_r_squared_y  10years_fund_std_y  \\\n",
       "0           4.639670e-02                  0.985255           -1.612714   \n",
       "1          -5.463049e-02                 -0.322354            0.104214   \n",
       "2          -1.880000e-17                  0.000000            0.000000   \n",
       "3          -1.880000e-17                  0.000000            0.000000   \n",
       "4          -1.721224e-01                  0.312962            0.702853   \n",
       "\n",
       "   10yrs_sharpe_ratio_category_y  10_years_beta_fund_y  \\\n",
       "0                        0.21756              0.005285   \n",
       "1                        0.21756              0.018901   \n",
       "2                        0.21756              0.000000   \n",
       "3                        0.21756              0.000000   \n",
       "4                        0.21756              0.075465   \n",
       "\n",
       "   10yrs_treynor_ratio_fund_y  10_years_return_mean_annual_category_y  \\\n",
       "0                   -0.051421                                0.167234   \n",
       "1                    0.049533                                0.514846   \n",
       "2                    0.000000                                0.391881   \n",
       "3                    0.000000                                0.514846   \n",
       "4                    0.039072                                0.514846   \n",
       "\n",
       "   10yrs_treynor_ratio_category_y  ...  5_years_beta_fund_y  \\\n",
       "0                       -0.223817  ...        -4.716087e-02   \n",
       "1                       -0.069452  ...        -3.179938e-01   \n",
       "2                        0.002630  ...         9.006177e-02   \n",
       "3                       -0.110593  ...        -1.880000e-16   \n",
       "4                        0.025274  ...         3.590886e-01   \n",
       "\n",
       "   5yrs_treynor_ratio_fund_y  5_years_return_mean_annual_fund_y  \\\n",
       "0              -4.441731e-02                      -4.697990e-01   \n",
       "1               5.069990e-02                      -2.382270e-02   \n",
       "2               2.224050e-01                      -7.564980e-01   \n",
       "3               3.660000e-17                       3.540000e-16   \n",
       "4               2.726401e-01                      -7.208662e-03   \n",
       "\n",
       "   5_years_return_mean_annual_category_y  5yrs_treynor_ratio_category_y  \\\n",
       "0                              -0.770071                      -0.247703   \n",
       "1                               1.207049                       0.065172   \n",
       "2                              -0.770071                       0.070614   \n",
       "3                              -0.770071                      -0.021888   \n",
       "4                               1.207049                       1.333001   \n",
       "\n",
       "   5_years_return_fund_y  5_years_alpha_category_y  5_years_beta_category_y  \\\n",
       "0              -0.135509                  0.308098                 0.347165   \n",
       "1               0.135495                  0.038507                 0.262340   \n",
       "2              -0.411309                  0.687491                 0.281527   \n",
       "3              -0.986893                  0.308098                 0.347165   \n",
       "4               2.092479                  0.308098                 0.347165   \n",
       "\n",
       "   5years_category_std_y  5_years_return_category_y  \n",
       "0              -1.275893                  -0.469596  \n",
       "1               0.542512                   1.209095  \n",
       "2              -1.275893                  -0.798866  \n",
       "3              -0.669758                  -0.052128  \n",
       "4               0.744557                   1.911734  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = preprocessing.normalize(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = pd.DataFrame(X_test)\n",
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10years_category_r_squared_y</th>\n",
       "      <th>10yrs_sharpe_ratio_fund_y</th>\n",
       "      <th>10_years_alpha_fund_y</th>\n",
       "      <th>10years_fund_r_squared_y</th>\n",
       "      <th>10years_fund_std_y</th>\n",
       "      <th>10yrs_sharpe_ratio_category_y</th>\n",
       "      <th>10_years_beta_fund_y</th>\n",
       "      <th>10yrs_treynor_ratio_fund_y</th>\n",
       "      <th>10_years_return_mean_annual_category_y</th>\n",
       "      <th>10yrs_treynor_ratio_category_y</th>\n",
       "      <th>...</th>\n",
       "      <th>5_years_beta_fund_y</th>\n",
       "      <th>5yrs_treynor_ratio_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_fund_y</th>\n",
       "      <th>5_years_return_mean_annual_category_y</th>\n",
       "      <th>5yrs_treynor_ratio_category_y</th>\n",
       "      <th>5_years_return_fund_y</th>\n",
       "      <th>5_years_alpha_category_y</th>\n",
       "      <th>5_years_beta_category_y</th>\n",
       "      <th>5years_category_std_y</th>\n",
       "      <th>5_years_return_category_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.689945</td>\n",
       "      <td>7.662144e-01</td>\n",
       "      <td>4.639670e-02</td>\n",
       "      <td>0.985255</td>\n",
       "      <td>-1.612714</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>-0.051421</td>\n",
       "      <td>0.167234</td>\n",
       "      <td>-0.223817</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.716087e-02</td>\n",
       "      <td>-4.441731e-02</td>\n",
       "      <td>-4.697990e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.247703</td>\n",
       "      <td>-0.135509</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.469596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.281792</td>\n",
       "      <td>1.172025e-01</td>\n",
       "      <td>-5.463049e-02</td>\n",
       "      <td>-0.322354</td>\n",
       "      <td>0.104214</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.018901</td>\n",
       "      <td>0.049533</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.179938e-01</td>\n",
       "      <td>5.069990e-02</td>\n",
       "      <td>-2.382270e-02</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>0.065172</td>\n",
       "      <td>0.135495</td>\n",
       "      <td>0.038507</td>\n",
       "      <td>0.262340</td>\n",
       "      <td>0.542512</td>\n",
       "      <td>1.209095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.302511</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391881</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>...</td>\n",
       "      <td>9.006177e-02</td>\n",
       "      <td>2.224050e-01</td>\n",
       "      <td>-7.564980e-01</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>0.070614</td>\n",
       "      <td>-0.411309</td>\n",
       "      <td>0.687491</td>\n",
       "      <td>0.281527</td>\n",
       "      <td>-1.275893</td>\n",
       "      <td>-0.798866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.763179</td>\n",
       "      <td>-4.000000e-16</td>\n",
       "      <td>-1.880000e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>-0.110593</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.880000e-16</td>\n",
       "      <td>3.660000e-17</td>\n",
       "      <td>3.540000e-16</td>\n",
       "      <td>-0.770071</td>\n",
       "      <td>-0.021888</td>\n",
       "      <td>-0.986893</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>-0.669758</td>\n",
       "      <td>-0.052128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.543477</td>\n",
       "      <td>4.417085e-01</td>\n",
       "      <td>-1.721224e-01</td>\n",
       "      <td>0.312962</td>\n",
       "      <td>0.702853</td>\n",
       "      <td>0.21756</td>\n",
       "      <td>0.075465</td>\n",
       "      <td>0.039072</td>\n",
       "      <td>0.514846</td>\n",
       "      <td>0.025274</td>\n",
       "      <td>...</td>\n",
       "      <td>3.590886e-01</td>\n",
       "      <td>2.726401e-01</td>\n",
       "      <td>-7.208662e-03</td>\n",
       "      <td>1.207049</td>\n",
       "      <td>1.333001</td>\n",
       "      <td>2.092479</td>\n",
       "      <td>0.308098</td>\n",
       "      <td>0.347165</td>\n",
       "      <td>0.744557</td>\n",
       "      <td>1.911734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10years_category_r_squared_y  10yrs_sharpe_ratio_fund_y  \\\n",
       "0                      0.689945               7.662144e-01   \n",
       "1                      0.281792               1.172025e-01   \n",
       "2                      0.302511              -4.000000e-16   \n",
       "3                      0.763179              -4.000000e-16   \n",
       "4                      0.543477               4.417085e-01   \n",
       "\n",
       "   10_years_alpha_fund_y  10years_fund_r_squared_y  10years_fund_std_y  \\\n",
       "0           4.639670e-02                  0.985255           -1.612714   \n",
       "1          -5.463049e-02                 -0.322354            0.104214   \n",
       "2          -1.880000e-17                  0.000000            0.000000   \n",
       "3          -1.880000e-17                  0.000000            0.000000   \n",
       "4          -1.721224e-01                  0.312962            0.702853   \n",
       "\n",
       "   10yrs_sharpe_ratio_category_y  10_years_beta_fund_y  \\\n",
       "0                        0.21756              0.005285   \n",
       "1                        0.21756              0.018901   \n",
       "2                        0.21756              0.000000   \n",
       "3                        0.21756              0.000000   \n",
       "4                        0.21756              0.075465   \n",
       "\n",
       "   10yrs_treynor_ratio_fund_y  10_years_return_mean_annual_category_y  \\\n",
       "0                   -0.051421                                0.167234   \n",
       "1                    0.049533                                0.514846   \n",
       "2                    0.000000                                0.391881   \n",
       "3                    0.000000                                0.514846   \n",
       "4                    0.039072                                0.514846   \n",
       "\n",
       "   10yrs_treynor_ratio_category_y  ...  5_years_beta_fund_y  \\\n",
       "0                       -0.223817  ...        -4.716087e-02   \n",
       "1                       -0.069452  ...        -3.179938e-01   \n",
       "2                        0.002630  ...         9.006177e-02   \n",
       "3                       -0.110593  ...        -1.880000e-16   \n",
       "4                        0.025274  ...         3.590886e-01   \n",
       "\n",
       "   5yrs_treynor_ratio_fund_y  5_years_return_mean_annual_fund_y  \\\n",
       "0              -4.441731e-02                      -4.697990e-01   \n",
       "1               5.069990e-02                      -2.382270e-02   \n",
       "2               2.224050e-01                      -7.564980e-01   \n",
       "3               3.660000e-17                       3.540000e-16   \n",
       "4               2.726401e-01                      -7.208662e-03   \n",
       "\n",
       "   5_years_return_mean_annual_category_y  5yrs_treynor_ratio_category_y  \\\n",
       "0                              -0.770071                      -0.247703   \n",
       "1                               1.207049                       0.065172   \n",
       "2                              -0.770071                       0.070614   \n",
       "3                              -0.770071                      -0.021888   \n",
       "4                               1.207049                       1.333001   \n",
       "\n",
       "   5_years_return_fund_y  5_years_alpha_category_y  5_years_beta_category_y  \\\n",
       "0              -0.135509                  0.308098                 0.347165   \n",
       "1               0.135495                  0.038507                 0.262340   \n",
       "2              -0.411309                  0.687491                 0.281527   \n",
       "3              -0.986893                  0.308098                 0.347165   \n",
       "4               2.092479                  0.308098                 0.347165   \n",
       "\n",
       "   5years_category_std_y  5_years_return_category_y  \n",
       "0              -1.275893                  -0.469596  \n",
       "1               0.542512                   1.209095  \n",
       "2              -1.275893                  -0.798866  \n",
       "3              -0.669758                  -0.052128  \n",
       "4               0.744557                   1.911734  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yb0Kx_ME6plD"
   },
   "source": [
    "### Basic NN model\n",
    "\n",
    "Naive MLP model without any alterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weHT53gF6aFY"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "izZlXqJu6sdo"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dCi-i296tuq"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(50, input_shape = (104, )))\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJ5FUE916x2b"
   },
   "outputs": [],
   "source": [
    "# sgd = optimizers.SGD(lr = 0.01)\n",
    "# model.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 116846,
     "status": "ok",
     "timestamp": 1574343017363,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "Zl34l0jR6zH-",
    "outputId": "b97b934c-c534-466f-832b-e84bb3ed402c"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 100, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2258,
     "status": "ok",
     "timestamp": 1574343027837,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "Rhl0xqgR62ei",
    "outputId": "559121de-a55b-4c76-bf55-5a51c3b35372"
   },
   "outputs": [],
   "source": [
    "preds = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preds = argmax(to_categorical(x, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1574343030327,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "6PSheNKy66Iu",
    "outputId": "46ec5282-7334-4739-e8c2-e7deef4fdc65"
   },
   "outputs": [],
   "source": [
    "submission_file = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file['greatstone_rating'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file.to_csv('Submission11.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V7kN5-f7IHv"
   },
   "source": [
    "### 1. Weight Initialization\n",
    "\n",
    "Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree\n",
    "\n",
    "Ref: https://keras.io/initializers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7il0ZpZZ67GP"
   },
   "outputs": [],
   "source": [
    "# from now on, create a function to generate (return) models\n",
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (104, ), kernel_initializer='he_normal'))     # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(Activation('sigmoid'))    \n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))                            # use he_normal initializer\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115349,
     "status": "ok",
     "timestamp": 1574343165332,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "9iOQeOzR7Q2O",
    "outputId": "3db57b76-306d-419d-8c74-5cff6828dba3"
   },
   "outputs": [],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, batch_size=200, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2178,
     "status": "ok",
     "timestamp": 1574343173561,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "ThB85Acn7SFu",
    "outputId": "be9d842b-15c4-44e3-fcae-f34fe4ac630d"
   },
   "outputs": [],
   "source": [
    "preds_he = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1574343173563,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "Iq5a7na67UgU",
    "outputId": "15ffc87a-3e71-4950-a9dd-1731ad0935d0"
   },
   "outputs": [],
   "source": [
    "submission_file = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission_file['greatstone_rating'] = preds_he\n",
    "\n",
    "submission_file.to_csv('Submission16.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5M6EHv17YUV"
   },
   "source": [
    "### 2. Nonlinearity (Activation function)\n",
    "\n",
    "Sigmoid functions suffer from gradient vanishing problem, making training slower\n",
    "\n",
    "There are many choices apart from sigmoid and tanh; try many of them!\n",
    "\n",
    "'relu' (rectified linear unit) is one of the most popular ones\n",
    "\n",
    "Ref: https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BuuRRvxL7WU1"
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (26, )))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(layers.Dense(6,Activation('softmax')))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68232,
     "status": "ok",
     "timestamp": 1574343243524,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "LHqen6477eRZ",
    "outputId": "442eca8d-254e-46cc-bfa3-aac557357473"
   },
   "outputs": [],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68336,
     "status": "ok",
     "timestamp": 1574343244956,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "LLrEe_Kn7fha",
    "outputId": "28e0bd0a-4cac-4c20-e3cd-ce5dbe1e47fe"
   },
   "outputs": [],
   "source": [
    "preds_rel = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67400,
     "status": "ok",
     "timestamp": 1574343244959,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "BBUqrqSd7heP",
    "outputId": "37637621-adef-4ddf-ebc4-9369a2d3d9aa"
   },
   "outputs": [],
   "source": [
    "submission_file = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission_file['greatstone_rating'] = preds_rel\n",
    "\n",
    "submission_file.to_csv('Submission13.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lp_-o2ug70fF"
   },
   "source": [
    "### 3. Batch Normalization\n",
    "\n",
    "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective\n",
    "\n",
    "Normalize each mini-batch before nonlinearity\n",
    "\n",
    "Ref: https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buhn2kOY7yg8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YrSd0LT78Yi"
   },
   "source": [
    "Batch normalization layer is usually inserted after dense/convolution and before nonlinearity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10InY0e_77M7"
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_shape = (26, )))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(50))\n",
    "    model.add(BatchNormalization())                    \n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dense(10))\n",
    "    model.add(layers.Dense(6,Activation('softmax')))\n",
    "    \n",
    "    sgd = optimizers.SGD(lr = 0.001)\n",
    "    model.compile(optimizer = sgd, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 250932,
     "status": "ok",
     "timestamp": 1574343442937,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "jOCywkTg79_W",
    "outputId": "4c44b42d-779a-4d48-a84c-50e7559d0ec8"
   },
   "outputs": [],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 20, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251917,
     "status": "ok",
     "timestamp": 1574343444719,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "gGodrhsc8AW7",
    "outputId": "d0458970-8e39-4465-e9c6-6f9c6da65536"
   },
   "outputs": [],
   "source": [
    "pred_bn = model.predict_classes(X_test)\n",
    "\n",
    "submission_file = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission_file['greatstone_rating'] = pred_bn\n",
    "\n",
    "submission_file.to_csv('Submission14.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251187,
     "status": "ok",
     "timestamp": 1574343444723,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "zDnDxm8r8Bt0",
    "outputId": "25bc9ed6-6be5-4f32-eb17-319a02da3a68"
   },
   "outputs": [],
   "source": [
    "print('Test accuracy: ', results[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElSuYRBH8yim"
   },
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKXR7REP8C8s"
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(500, input_shape = (104, ), kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(500, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.01))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(layers.Dense(6,Activation('softmax')))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 367826,
     "status": "ok",
     "timestamp": 1574343564038,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "8FJsCqh680bq",
    "outputId": "9eaecfba-85ca-4653-9190-2fac675b7231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples\n",
      "Epoch 1/300\n",
      "16000/16000 [==============================] - 9s 538us/sample - loss: 0.3821 - accuracy: 0.8302\n",
      "Epoch 2/300\n",
      "16000/16000 [==============================] - 7s 429us/sample - loss: 0.3009 - accuracy: 0.8521\n",
      "Epoch 3/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.2779 - accuracy: 0.8627\n",
      "Epoch 4/300\n",
      "16000/16000 [==============================] - 6s 393us/sample - loss: 0.2680 - accuracy: 0.8685\n",
      "Epoch 5/300\n",
      "16000/16000 [==============================] - 6s 406us/sample - loss: 0.2590 - accuracy: 0.8736 - loss:\n",
      "Epoch 6/300\n",
      "16000/16000 [==============================] - 7s 414us/sample - loss: 0.2524 - accuracy: 0.8779\n",
      "Epoch 7/300\n",
      "16000/16000 [==============================] - 7s 431us/sample - loss: 0.2463 - accuracy: 0.8823\n",
      "Epoch 8/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.2421 - accuracy: 0.8835\n",
      "Epoch 9/300\n",
      "16000/16000 [==============================] - 6s 399us/sample - loss: 0.2372 - accuracy: 0.8873\n",
      "Epoch 10/300\n",
      "16000/16000 [==============================] - 6s 388us/sample - loss: 0.2345 - accuracy: 0.8892\n",
      "Epoch 11/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.2314 - accuracy: 0.8912\n",
      "Epoch 12/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.2290 - accuracy: 0.8934\n",
      "Epoch 13/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.2256 - accuracy: 0.8945 - l\n",
      "Epoch 14/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.2234 - accuracy: 0.8957\n",
      "Epoch 15/300\n",
      "16000/16000 [==============================] - 7s 410us/sample - loss: 0.2207 - accuracy: 0.8981\n",
      "Epoch 16/300\n",
      "16000/16000 [==============================] - 7s 411us/sample - loss: 0.2171 - accuracy: 0.8996\n",
      "Epoch 17/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.2175 - accuracy: 0.9000\n",
      "Epoch 18/300\n",
      "16000/16000 [==============================] - 6s 404us/sample - loss: 0.2148 - accuracy: 0.9010 - loss: 0\n",
      "Epoch 19/300\n",
      "16000/16000 [==============================] - 7s 410us/sample - loss: 0.2141 - accuracy: 0.9013\n",
      "Epoch 20/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.2082 - accuracy: 0.9057\n",
      "Epoch 21/300\n",
      "16000/16000 [==============================] - 6s 390us/sample - loss: 0.2052 - accuracy: 0.9075\n",
      "Epoch 22/300\n",
      "16000/16000 [==============================] - 6s 389us/sample - loss: 0.2062 - accuracy: 0.9059\n",
      "Epoch 23/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.2006 - accuracy: 0.9090\n",
      "Epoch 24/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.2043 - accuracy: 0.9076\n",
      "Epoch 25/300\n",
      "16000/16000 [==============================] - 6s 388us/sample - loss: 0.1988 - accuracy: 0.9106\n",
      "Epoch 26/300\n",
      "16000/16000 [==============================] - 6s 405us/sample - loss: 0.1948 - accuracy: 0.9124\n",
      "Epoch 27/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1968 - accuracy: 0.9101\n",
      "Epoch 28/300\n",
      "16000/16000 [==============================] - 6s 398us/sample - loss: 0.1891 - accuracy: 0.9142\n",
      "Epoch 29/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1879 - accuracy: 0.9180\n",
      "Epoch 30/300\n",
      "16000/16000 [==============================] - 6s 395us/sample - loss: 0.1919 - accuracy: 0.9156 -\n",
      "Epoch 31/300\n",
      "16000/16000 [==============================] - 6s 393us/sample - loss: 0.1914 - accuracy: 0.9155\n",
      "Epoch 32/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.1840 - accuracy: 0.9187\n",
      "Epoch 33/300\n",
      "16000/16000 [==============================] - 6s 394us/sample - loss: 0.1860 - accuracy: 0.9177\n",
      "Epoch 34/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1843 - accuracy: 0.9164\n",
      "Epoch 35/300\n",
      "16000/16000 [==============================] - 6s 394us/sample - loss: 0.1801 - accuracy: 0.9205\n",
      "Epoch 36/300\n",
      "16000/16000 [==============================] - 6s 405us/sample - loss: 0.1813 - accuracy: 0.9197\n",
      "Epoch 37/300\n",
      "16000/16000 [==============================] - 7s 419us/sample - loss: 0.1792 - accuracy: 0.9206 - loss: 0.1\n",
      "Epoch 38/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.1779 - accuracy: 0.9213\n",
      "Epoch 39/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1750 - accuracy: 0.9228\n",
      "Epoch 40/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1742 - accuracy: 0.9230\n",
      "Epoch 41/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.1721 - accuracy: 0.9248\n",
      "Epoch 42/300\n",
      "16000/16000 [==============================] - 6s 400us/sample - loss: 0.1726 - accuracy: 0.9240\n",
      "Epoch 43/300\n",
      "16000/16000 [==============================] - 6s 399us/sample - loss: 0.1709 - accuracy: 0.9256\n",
      "Epoch 44/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.1688 - accuracy: 0.9273\n",
      "Epoch 45/300\n",
      "16000/16000 [==============================] - 7s 419us/sample - loss: 0.1700 - accuracy: 0.9264\n",
      "Epoch 46/300\n",
      "16000/16000 [==============================] - 7s 432us/sample - loss: 0.1696 - accuracy: 0.9258\n",
      "Epoch 47/300\n",
      "16000/16000 [==============================] - 6s 399us/sample - loss: 0.1670 - accuracy: 0.9279\n",
      "Epoch 48/300\n",
      "16000/16000 [==============================] - 6s 405us/sample - loss: 0.1651 - accuracy: 0.9274\n",
      "Epoch 49/300\n",
      "16000/16000 [==============================] - 7s 408us/sample - loss: 0.1629 - accuracy: 0.9303\n",
      "Epoch 50/300\n",
      "16000/16000 [==============================] - 7s 413us/sample - loss: 0.1604 - accuracy: 0.9295\n",
      "Epoch 51/300\n",
      "16000/16000 [==============================] - 7s 411us/sample - loss: 0.1615 - accuracy: 0.9313\n",
      "Epoch 52/300\n",
      "16000/16000 [==============================] - 7s 408us/sample - loss: 0.1605 - accuracy: 0.9307\n",
      "Epoch 53/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1599 - accuracy: 0.9321\n",
      "Epoch 54/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1562 - accuracy: 0.9326\n",
      "Epoch 55/300\n",
      "16000/16000 [==============================] - 6s 403us/sample - loss: 0.1577 - accuracy: 0.9315\n",
      "Epoch 56/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.1555 - accuracy: 0.9327\n",
      "Epoch 57/300\n",
      "16000/16000 [==============================] - 6s 399us/sample - loss: 0.1550 - accuracy: 0.9334\n",
      "Epoch 58/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1539 - accuracy: 0.9338\n",
      "Epoch 59/300\n",
      "16000/16000 [==============================] - 6s 398us/sample - loss: 0.1524 - accuracy: 0.9349\n",
      "Epoch 60/300\n",
      "16000/16000 [==============================] - 6s 400us/sample - loss: 0.1492 - accuracy: 0.9365\n",
      "Epoch 61/300\n",
      "16000/16000 [==============================] - 6s 403us/sample - loss: 0.1556 - accuracy: 0.9347\n",
      "Epoch 62/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1481 - accuracy: 0.9367\n",
      "Epoch 63/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.1498 - accuracy: 0.9369\n",
      "Epoch 64/300\n",
      "16000/16000 [==============================] - 7s 411us/sample - loss: 0.1519 - accuracy: 0.9357\n",
      "Epoch 65/300\n",
      "16000/16000 [==============================] - 7s 411us/sample - loss: 0.1499 - accuracy: 0.9367\n",
      "Epoch 66/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.1468 - accuracy: 0.9372\n",
      "Epoch 67/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1441 - accuracy: 0.9391\n",
      "Epoch 68/300\n",
      "16000/16000 [==============================] - 6s 395us/sample - loss: 0.1435 - accuracy: 0.9391\n",
      "Epoch 69/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1444 - accuracy: 0.9399\n",
      "Epoch 70/300\n",
      "16000/16000 [==============================] - 6s 405us/sample - loss: 0.1419 - accuracy: 0.9403\n",
      "Epoch 71/300\n",
      "16000/16000 [==============================] - 6s 400us/sample - loss: 0.1421 - accuracy: 0.9396\n",
      "Epoch 72/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1393 - accuracy: 0.9409 - loss: 0.1395 - accuracy: 0.\n",
      "Epoch 73/300\n",
      "16000/16000 [==============================] - 7s 414us/sample - loss: 0.1419 - accuracy: 0.9396\n",
      "Epoch 74/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 6s 395us/sample - loss: 0.1406 - accuracy: 0.9405\n",
      "Epoch 75/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.1374 - accuracy: 0.9424\n",
      "Epoch 76/300\n",
      "16000/16000 [==============================] - 7s 409us/sample - loss: 0.1389 - accuracy: 0.9422\n",
      "Epoch 77/300\n",
      "16000/16000 [==============================] - 6s 405us/sample - loss: 0.1380 - accuracy: 0.9421\n",
      "Epoch 78/300\n",
      "16000/16000 [==============================] - 6s 392us/sample - loss: 0.1375 - accuracy: 0.9413\n",
      "Epoch 79/300\n",
      "16000/16000 [==============================] - 7s 408us/sample - loss: 0.1372 - accuracy: 0.9425\n",
      "Epoch 80/300\n",
      "16000/16000 [==============================] - 7s 409us/sample - loss: 0.1386 - accuracy: 0.9411\n",
      "Epoch 81/300\n",
      "16000/16000 [==============================] - 6s 395us/sample - loss: 0.1365 - accuracy: 0.9422\n",
      "Epoch 82/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.1358 - accuracy: 0.9429\n",
      "Epoch 83/300\n",
      "16000/16000 [==============================] - 7s 407us/sample - loss: 0.1330 - accuracy: 0.9447\n",
      "Epoch 84/300\n",
      "16000/16000 [==============================] - 7s 412us/sample - loss: 0.1289 - accuracy: 0.9457\n",
      "Epoch 85/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.1297 - accuracy: 0.9465\n",
      "Epoch 86/300\n",
      "16000/16000 [==============================] - 6s 390us/sample - loss: 0.1327 - accuracy: 0.9446\n",
      "Epoch 87/300\n",
      "16000/16000 [==============================] - 6s 390us/sample - loss: 0.1316 - accuracy: 0.9463\n",
      "Epoch 88/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.1283 - accuracy: 0.9474\n",
      "Epoch 89/300\n",
      "16000/16000 [==============================] - 6s 387us/sample - loss: 0.1279 - accuracy: 0.9475\n",
      "Epoch 90/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.1295 - accuracy: 0.9464\n",
      "Epoch 91/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.1286 - accuracy: 0.9468\n",
      "Epoch 92/300\n",
      "16000/16000 [==============================] - 6s 394us/sample - loss: 0.1266 - accuracy: 0.9481\n",
      "Epoch 93/300\n",
      "16000/16000 [==============================] - 6s 401us/sample - loss: 0.1280 - accuracy: 0.9468\n",
      "Epoch 94/300\n",
      "16000/16000 [==============================] - 6s 393us/sample - loss: 0.1262 - accuracy: 0.9479\n",
      "Epoch 95/300\n",
      "16000/16000 [==============================] - 6s 392us/sample - loss: 0.1246 - accuracy: 0.9482\n",
      "Epoch 96/300\n",
      "16000/16000 [==============================] - 6s 403us/sample - loss: 0.1306 - accuracy: 0.9459\n",
      "Epoch 97/300\n",
      "16000/16000 [==============================] - 6s 399us/sample - loss: 0.1264 - accuracy: 0.9486\n",
      "Epoch 98/300\n",
      "16000/16000 [==============================] - 6s 395us/sample - loss: 0.1233 - accuracy: 0.9492\n",
      "Epoch 99/300\n",
      "16000/16000 [==============================] - 6s 395us/sample - loss: 0.1254 - accuracy: 0.9483\n",
      "Epoch 100/300\n",
      "16000/16000 [==============================] - 6s 394us/sample - loss: 0.1211 - accuracy: 0.9495\n",
      "Epoch 101/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1227 - accuracy: 0.9500\n",
      "Epoch 102/300\n",
      "16000/16000 [==============================] - 6s 405us/sample - loss: 0.1244 - accuracy: 0.9492\n",
      "Epoch 103/300\n",
      "16000/16000 [==============================] - 6s 394us/sample - loss: 0.1228 - accuracy: 0.9504\n",
      "Epoch 104/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.1191 - accuracy: 0.9518\n",
      "Epoch 105/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.1226 - accuracy: 0.9496 - loss:\n",
      "Epoch 106/300\n",
      "16000/16000 [==============================] - 6s 392us/sample - loss: 0.1219 - accuracy: 0.9503 - loss: 0.121\n",
      "Epoch 107/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.1192 - accuracy: 0.9518\n",
      "Epoch 108/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.1213 - accuracy: 0.9501\n",
      "Epoch 109/300\n",
      "16000/16000 [==============================] - 6s 391us/sample - loss: 0.1167 - accuracy: 0.9522\n",
      "Epoch 110/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1173 - accuracy: 0.9528\n",
      "Epoch 111/300\n",
      "16000/16000 [==============================] - 6s 393us/sample - loss: 0.1189 - accuracy: 0.9517\n",
      "Epoch 112/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1190 - accuracy: 0.9527\n",
      "Epoch 113/300\n",
      "16000/16000 [==============================] - 6s 394us/sample - loss: 0.1159 - accuracy: 0.9527\n",
      "Epoch 114/300\n",
      "16000/16000 [==============================] - 6s 396us/sample - loss: 0.1206 - accuracy: 0.9506\n",
      "Epoch 115/300\n",
      "16000/16000 [==============================] - 6s 397us/sample - loss: 0.1171 - accuracy: 0.9532\n",
      "Epoch 116/300\n",
      "16000/16000 [==============================] - 6s 395us/sample - loss: 0.1169 - accuracy: 0.9528\n",
      "Epoch 117/300\n",
      "16000/16000 [==============================] - 7s 442us/sample - loss: 0.1130 - accuracy: 0.9540\n",
      "Epoch 118/300\n",
      "16000/16000 [==============================] - 7s 467us/sample - loss: 0.1175 - accuracy: 0.9523\n",
      "Epoch 119/300\n",
      "16000/16000 [==============================] - 8s 472us/sample - loss: 0.1122 - accuracy: 0.9544\n",
      "Epoch 120/300\n",
      "16000/16000 [==============================] - 7s 467us/sample - loss: 0.1157 - accuracy: 0.9531\n",
      "Epoch 121/300\n",
      "16000/16000 [==============================] - 7s 465us/sample - loss: 0.1110 - accuracy: 0.9553\n",
      "Epoch 122/300\n",
      "16000/16000 [==============================] - 8s 470us/sample - loss: 0.1124 - accuracy: 0.9542\n",
      "Epoch 123/300\n",
      "16000/16000 [==============================] - 7s 438us/sample - loss: 0.1130 - accuracy: 0.9542\n",
      "Epoch 124/300\n",
      "16000/16000 [==============================] - 6s 362us/sample - loss: 0.1116 - accuracy: 0.9541\n",
      "Epoch 125/300\n",
      "16000/16000 [==============================] - 6s 376us/sample - loss: 0.1109 - accuracy: 0.9548\n",
      "Epoch 126/300\n",
      "16000/16000 [==============================] - 6s 361us/sample - loss: 0.1152 - accuracy: 0.9532\n",
      "Epoch 127/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.1137 - accuracy: 0.9538\n",
      "Epoch 128/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.1111 - accuracy: 0.9547\n",
      "Epoch 129/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.1119 - accuracy: 0.9547\n",
      "Epoch 130/300\n",
      "16000/16000 [==============================] - 6s 354us/sample - loss: 0.1088 - accuracy: 0.9568\n",
      "Epoch 131/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.1109 - accuracy: 0.9552\n",
      "Epoch 132/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.1090 - accuracy: 0.9557\n",
      "Epoch 133/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.1084 - accuracy: 0.9555\n",
      "Epoch 134/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.1089 - accuracy: 0.9559\n",
      "Epoch 135/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.1093 - accuracy: 0.9554\n",
      "Epoch 136/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.1059 - accuracy: 0.9576\n",
      "Epoch 137/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.1091 - accuracy: 0.9561\n",
      "Epoch 138/300\n",
      "16000/16000 [==============================] - 6s 352us/sample - loss: 0.1037 - accuracy: 0.9580\n",
      "Epoch 139/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.1036 - accuracy: 0.9581\n",
      "Epoch 140/300\n",
      "16000/16000 [==============================] - 6s 356us/sample - loss: 0.1094 - accuracy: 0.9560\n",
      "Epoch 141/300\n",
      "16000/16000 [==============================] - 6s 356us/sample - loss: 0.1072 - accuracy: 0.9572\n",
      "Epoch 142/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.1060 - accuracy: 0.9578 - loss: 0.1056 - accuracy: 0.\n",
      "Epoch 143/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.1025 - accuracy: 0.9588\n",
      "Epoch 144/300\n",
      "16000/16000 [==============================] - 6s 356us/sample - loss: 0.1053 - accuracy: 0.9578\n",
      "Epoch 145/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.1066 - accuracy: 0.9572\n",
      "Epoch 146/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.1052 - accuracy: 0.9577\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.1043 - accuracy: 0.9589\n",
      "Epoch 148/300\n",
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.1084 - accuracy: 0.9563\n",
      "Epoch 149/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.1038 - accuracy: 0.9580\n",
      "Epoch 150/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.1049 - accuracy: 0.9576\n",
      "Epoch 151/300\n",
      "16000/16000 [==============================] - 5s 344us/sample - loss: 0.1012 - accuracy: 0.9594\n",
      "Epoch 152/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.1026 - accuracy: 0.9587\n",
      "Epoch 153/300\n",
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.0981 - accuracy: 0.9620\n",
      "Epoch 154/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.1072 - accuracy: 0.9572\n",
      "Epoch 155/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.1012 - accuracy: 0.9602\n",
      "Epoch 156/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.1021 - accuracy: 0.9597\n",
      "Epoch 157/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.1010 - accuracy: 0.9590\n",
      "Epoch 158/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.1002 - accuracy: 0.9608\n",
      "Epoch 159/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0991 - accuracy: 0.9604\n",
      "Epoch 160/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.0956 - accuracy: 0.9632\n",
      "Epoch 161/300\n",
      "16000/16000 [==============================] - 6s 355us/sample - loss: 0.0993 - accuracy: 0.9603\n",
      "Epoch 162/300\n",
      "16000/16000 [==============================] - 6s 368us/sample - loss: 0.0976 - accuracy: 0.9612\n",
      "Epoch 163/300\n",
      "16000/16000 [==============================] - 6s 360us/sample - loss: 0.0985 - accuracy: 0.9618\n",
      "Epoch 164/300\n",
      "16000/16000 [==============================] - 6s 360us/sample - loss: 0.0985 - accuracy: 0.9610\n",
      "Epoch 165/300\n",
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.0997 - accuracy: 0.9605\n",
      "Epoch 166/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.0954 - accuracy: 0.9612\n",
      "Epoch 167/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0979 - accuracy: 0.9608\n",
      "Epoch 168/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0967 - accuracy: 0.9618\n",
      "Epoch 169/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.1005 - accuracy: 0.9609\n",
      "Epoch 170/300\n",
      "16000/16000 [==============================] - 5s 344us/sample - loss: 0.0992 - accuracy: 0.9606\n",
      "Epoch 171/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.0965 - accuracy: 0.9610\n",
      "Epoch 172/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0974 - accuracy: 0.9620\n",
      "Epoch 173/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0964 - accuracy: 0.9617\n",
      "Epoch 174/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0988 - accuracy: 0.9602\n",
      "Epoch 175/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0956 - accuracy: 0.9617\n",
      "Epoch 176/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0949 - accuracy: 0.9625\n",
      "Epoch 177/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.0947 - accuracy: 0.9632\n",
      "Epoch 178/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0969 - accuracy: 0.9611\n",
      "Epoch 179/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0957 - accuracy: 0.9618\n",
      "Epoch 180/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0964 - accuracy: 0.9610\n",
      "Epoch 181/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0902 - accuracy: 0.9639\n",
      "Epoch 182/300\n",
      "16000/16000 [==============================] - 6s 352us/sample - loss: 0.0959 - accuracy: 0.9631\n",
      "Epoch 183/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.0946 - accuracy: 0.9622\n",
      "Epoch 184/300\n",
      "16000/16000 [==============================] - 6s 363us/sample - loss: 0.0945 - accuracy: 0.9631\n",
      "Epoch 185/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0941 - accuracy: 0.9631\n",
      "Epoch 186/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0927 - accuracy: 0.9633\n",
      "Epoch 187/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0933 - accuracy: 0.9635\n",
      "Epoch 188/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0936 - accuracy: 0.9631\n",
      "Epoch 189/300\n",
      "16000/16000 [==============================] - 6s 354us/sample - loss: 0.0931 - accuracy: 0.9624\n",
      "Epoch 190/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0939 - accuracy: 0.9634\n",
      "Epoch 191/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0932 - accuracy: 0.9630\n",
      "Epoch 192/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0936 - accuracy: 0.9632\n",
      "Epoch 193/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0921 - accuracy: 0.9630\n",
      "Epoch 194/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0932 - accuracy: 0.9625\n",
      "Epoch 195/300\n",
      "16000/16000 [==============================] - 6s 352us/sample - loss: 0.0921 - accuracy: 0.9644\n",
      "Epoch 196/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.0890 - accuracy: 0.9647\n",
      "Epoch 197/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0946 - accuracy: 0.9621\n",
      "Epoch 198/300\n",
      "16000/16000 [==============================] - 6s 357us/sample - loss: 0.0914 - accuracy: 0.9631\n",
      "Epoch 199/300\n",
      "16000/16000 [==============================] - 6s 379us/sample - loss: 0.0932 - accuracy: 0.9632\n",
      "Epoch 200/300\n",
      "16000/16000 [==============================] - 6s 374us/sample - loss: 0.0881 - accuracy: 0.9656\n",
      "Epoch 201/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0884 - accuracy: 0.9649\n",
      "Epoch 202/300\n",
      "16000/16000 [==============================] - 6s 373us/sample - loss: 0.0917 - accuracy: 0.9637\n",
      "Epoch 203/300\n",
      "16000/16000 [==============================] - 6s 389us/sample - loss: 0.0930 - accuracy: 0.9628\n",
      "Epoch 204/300\n",
      "16000/16000 [==============================] - 6s 355us/sample - loss: 0.0894 - accuracy: 0.9656\n",
      "Epoch 205/300\n",
      "16000/16000 [==============================] - 6s 358us/sample - loss: 0.0888 - accuracy: 0.9646\n",
      "Epoch 206/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.0894 - accuracy: 0.9647\n",
      "Epoch 207/300\n",
      "16000/16000 [==============================] - 6s 356us/sample - loss: 0.0899 - accuracy: 0.9644\n",
      "Epoch 208/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0915 - accuracy: 0.9642\n",
      "Epoch 209/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.0873 - accuracy: 0.9656\n",
      "Epoch 210/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0907 - accuracy: 0.9638\n",
      "Epoch 211/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.0869 - accuracy: 0.9658\n",
      "Epoch 212/300\n",
      "16000/16000 [==============================] - 6s 354us/sample - loss: 0.0898 - accuracy: 0.9650\n",
      "Epoch 213/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0881 - accuracy: 0.9650\n",
      "Epoch 214/300\n",
      "16000/16000 [==============================] - 7s 408us/sample - loss: 0.0868 - accuracy: 0.9659\n",
      "Epoch 215/300\n",
      "16000/16000 [==============================] - 7s 446us/sample - loss: 0.0860 - accuracy: 0.9656\n",
      "Epoch 216/300\n",
      "16000/16000 [==============================] - 7s 435us/sample - loss: 0.0871 - accuracy: 0.9660\n",
      "Epoch 217/300\n",
      "16000/16000 [==============================] - 7s 418us/sample - loss: 0.0867 - accuracy: 0.9657\n",
      "Epoch 218/300\n",
      "16000/16000 [==============================] - 6s 402us/sample - loss: 0.0884 - accuracy: 0.9655\n",
      "Epoch 219/300\n",
      "16000/16000 [==============================] - 6s 405us/sample - loss: 0.0883 - accuracy: 0.9659\n",
      "Epoch 220/300\n",
      "16000/16000 [==============================] - 6s 368us/sample - loss: 0.0866 - accuracy: 0.9653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0887 - accuracy: 0.9655\n",
      "Epoch 222/300\n",
      "16000/16000 [==============================] - 5s 341us/sample - loss: 0.0867 - accuracy: 0.9667\n",
      "Epoch 223/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0854 - accuracy: 0.9661\n",
      "Epoch 224/300\n",
      "16000/16000 [==============================] - 5s 344us/sample - loss: 0.0848 - accuracy: 0.9673\n",
      "Epoch 225/300\n",
      "16000/16000 [==============================] - 6s 358us/sample - loss: 0.0884 - accuracy: 0.9654\n",
      "Epoch 226/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0847 - accuracy: 0.9665\n",
      "Epoch 227/300\n",
      "16000/16000 [==============================] - 6s 360us/sample - loss: 0.0847 - accuracy: 0.9672\n",
      "Epoch 228/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0852 - accuracy: 0.9662\n",
      "Epoch 229/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.0863 - accuracy: 0.9655\n",
      "Epoch 230/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0835 - accuracy: 0.9685\n",
      "Epoch 231/300\n",
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.0866 - accuracy: 0.9662\n",
      "Epoch 232/300\n",
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.0841 - accuracy: 0.9669\n",
      "Epoch 233/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0844 - accuracy: 0.9663\n",
      "Epoch 234/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.0828 - accuracy: 0.9671\n",
      "Epoch 235/300\n",
      "16000/16000 [==============================] - 5s 343us/sample - loss: 0.0847 - accuracy: 0.9669\n",
      "Epoch 236/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0837 - accuracy: 0.9678\n",
      "Epoch 237/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0875 - accuracy: 0.9655\n",
      "Epoch 238/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0859 - accuracy: 0.9659\n",
      "Epoch 239/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0830 - accuracy: 0.9677\n",
      "Epoch 240/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0832 - accuracy: 0.9677\n",
      "Epoch 241/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0825 - accuracy: 0.9673\n",
      "Epoch 242/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0804 - accuracy: 0.9685\n",
      "Epoch 243/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0847 - accuracy: 0.9673\n",
      "Epoch 244/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0835 - accuracy: 0.9686\n",
      "Epoch 245/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0816 - accuracy: 0.9683\n",
      "Epoch 246/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0840 - accuracy: 0.9665\n",
      "Epoch 247/300\n",
      "16000/16000 [==============================] - 6s 355us/sample - loss: 0.0822 - accuracy: 0.9678\n",
      "Epoch 248/300\n",
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0814 - accuracy: 0.9683\n",
      "Epoch 249/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0831 - accuracy: 0.9673\n",
      "Epoch 250/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0848 - accuracy: 0.9667\n",
      "Epoch 251/300\n",
      "16000/16000 [==============================] - 6s 378us/sample - loss: 0.0783 - accuracy: 0.9688\n",
      "Epoch 252/300\n",
      "16000/16000 [==============================] - 6s 361us/sample - loss: 0.0816 - accuracy: 0.9683\n",
      "Epoch 253/300\n",
      "16000/16000 [==============================] - 6s 365us/sample - loss: 0.0821 - accuracy: 0.9678\n",
      "Epoch 254/300\n",
      "16000/16000 [==============================] - 6s 354us/sample - loss: 0.0835 - accuracy: 0.9679\n",
      "Epoch 255/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0786 - accuracy: 0.9697\n",
      "Epoch 256/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0824 - accuracy: 0.9676\n",
      "Epoch 257/300\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.96 - 6s 346us/sample - loss: 0.0812 - accuracy: 0.9686\n",
      "Epoch 258/300\n",
      "16000/16000 [==============================] - 6s 378us/sample - loss: 0.0828 - accuracy: 0.9677\n",
      "Epoch 259/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0799 - accuracy: 0.9686\n",
      "Epoch 260/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0805 - accuracy: 0.9687\n",
      "Epoch 261/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0780 - accuracy: 0.9697\n",
      "Epoch 262/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0799 - accuracy: 0.9692\n",
      "Epoch 263/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0790 - accuracy: 0.9694\n",
      "Epoch 264/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.0819 - accuracy: 0.9685\n",
      "Epoch 265/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0788 - accuracy: 0.9694\n",
      "Epoch 266/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0785 - accuracy: 0.9696\n",
      "Epoch 267/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0789 - accuracy: 0.9693\n",
      "Epoch 268/300\n",
      "16000/16000 [==============================] - 6s 360us/sample - loss: 0.0783 - accuracy: 0.9695\n",
      "Epoch 269/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.0789 - accuracy: 0.9696\n",
      "Epoch 270/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.0769 - accuracy: 0.9694\n",
      "Epoch 271/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0799 - accuracy: 0.9687\n",
      "Epoch 272/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0789 - accuracy: 0.9694\n",
      "Epoch 273/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0786 - accuracy: 0.9701\n",
      "Epoch 274/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.0798 - accuracy: 0.9683\n",
      "Epoch 275/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.0762 - accuracy: 0.9700\n",
      "Epoch 276/300\n",
      "16000/16000 [==============================] - 6s 347us/sample - loss: 0.0777 - accuracy: 0.9699\n",
      "Epoch 277/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0766 - accuracy: 0.9693\n",
      "Epoch 278/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0777 - accuracy: 0.9697\n",
      "Epoch 279/300\n",
      "16000/16000 [==============================] - 6s 360us/sample - loss: 0.0781 - accuracy: 0.9707\n",
      "Epoch 280/300\n",
      "16000/16000 [==============================] - 6s 348us/sample - loss: 0.0772 - accuracy: 0.9696\n",
      "Epoch 281/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0764 - accuracy: 0.9702\n",
      "Epoch 282/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0763 - accuracy: 0.9701\n",
      "Epoch 283/300\n",
      "16000/16000 [==============================] - 6s 352us/sample - loss: 0.0767 - accuracy: 0.9708 - los\n",
      "Epoch 284/300\n",
      "16000/16000 [==============================] - 6s 350us/sample - loss: 0.0775 - accuracy: 0.9705\n",
      "Epoch 285/300\n",
      "16000/16000 [==============================] - 6s 381us/sample - loss: 0.0741 - accuracy: 0.9709\n",
      "Epoch 286/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0742 - accuracy: 0.9718\n",
      "Epoch 287/300\n",
      "16000/16000 [==============================] - 6s 378us/sample - loss: 0.0767 - accuracy: 0.9706\n",
      "Epoch 288/300\n",
      "16000/16000 [==============================] - 6s 370us/sample - loss: 0.0773 - accuracy: 0.9706\n",
      "Epoch 289/300\n",
      "16000/16000 [==============================] - 6s 359us/sample - loss: 0.0758 - accuracy: 0.9704\n",
      "Epoch 290/300\n",
      "16000/16000 [==============================] - 6s 355us/sample - loss: 0.0737 - accuracy: 0.9715\n",
      "Epoch 291/300\n",
      "16000/16000 [==============================] - 6s 352us/sample - loss: 0.0753 - accuracy: 0.9704\n",
      "Epoch 292/300\n",
      "16000/16000 [==============================] - 6s 352us/sample - loss: 0.0755 - accuracy: 0.9717\n",
      "Epoch 293/300\n",
      "16000/16000 [==============================] - 6s 351us/sample - loss: 0.0753 - accuracy: 0.9712\n",
      "Epoch 294/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 6s 344us/sample - loss: 0.0761 - accuracy: 0.9711\n",
      "Epoch 295/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0762 - accuracy: 0.9706\n",
      "Epoch 296/300\n",
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.0723 - accuracy: 0.9722\n",
      "Epoch 297/300\n",
      "16000/16000 [==============================] - 6s 345us/sample - loss: 0.0787 - accuracy: 0.9694\n",
      "Epoch 298/300\n",
      "16000/16000 [==============================] - 6s 346us/sample - loss: 0.0733 - accuracy: 0.9720\n",
      "Epoch 299/300\n",
      "16000/16000 [==============================] - 5s 342us/sample - loss: 0.0736 - accuracy: 0.9717\n",
      "Epoch 300/300\n",
      "16000/16000 [==============================] - 6s 349us/sample - loss: 0.0724 - accuracy: 0.9724\n"
     ]
    }
   ],
   "source": [
    "model = mlp_model()\n",
    "history = model.fit(X_train, y_train, epochs = 300, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 368025,
     "status": "ok",
     "timestamp": 1574343565750,
     "user": {
      "displayName": "Suryansh Sharma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBy1CGO0PDyRG1nsJ6rDaUXBg-h18OkEJ86htszrw=s64",
      "userId": "03232072030227591914"
     },
     "user_tz": -330
    },
    "id": "SU5NUjzs85KT",
    "outputId": "91bd9ae0-9b8e-41f6-fab7-3f19553580aa"
   },
   "outputs": [],
   "source": [
    "validation_results = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 1s 176us/sample - loss: 0.2000 - accuracy: 0.9301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1999654276371002, 0.930125]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "submission_file['greatstone_rating'] = results\n",
    "\n",
    "submission_file.to_csv('Submission39.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image_Classification_Intro_to_NN_3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
